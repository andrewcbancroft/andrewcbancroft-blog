---
title: "Stages to Machine Learning"
toc: true
description: "What does it take to really do machine learning? Explore these 10 stages to an effective machine learning process."
---
![One does not simply... machine learn]({{ "/assets/one-does-not-simply-machine-learn.png" | relative_url }})

If one does not simply... "machine learn"... what *does* one do to cover all the bases and make sure that the process is complete and value-producing?

Here 10 high level stages to check off as you take on a machine learning task.

## Blend
*More data*.  That always seems to be the answer to our predictive woes, doesn't it?

Two data sets augmenting one another lend more knowledge for machine learning algorithms to latch on to and figure out patterns within.

You could use the word "integrate" here if you want.  The point is that you're gathering data from multiple sources to help give the most complete picture of what you're analyzing and predicting.

## Explore
To make sense of the data you've got, especially if you didn't generate it in the first place, you need an exploration process built in to your machine learning pipeline.

Profile your data -- find out the good parts and the bad (dirty) parts, such as missing values, inconsistent labels, etc.

Compute summary statistics.

Visualize your data set by plotting the variables against one another to gain intuition about how the variables relate.

Use multiple kinds of visualizations to get a good feel for the "shape" of your data.

## Clean
Data is messy (pretty much always).

Somewhere in your machine learning pipeline, you've got to have a cleaning step where you fill in missing values, correct misspellings, etc.

Do the work to clean your data so that the predictions generated by machine learning are optimal.

## Transform
Data is in constant need of transformation, especially when it comes from multiple sources.

In order to get things consistent and clean, you often need to transform the values of your variables so that they're in conformity with one another.

For example, suppose that two data sets contain a column named "State".  In the first, the values look like this:
* OK
* NY
* FL

In the second, the values look like this:
* Oklahoma
* New York
* Florida

For conformity and consistency's sake, you need to pick a format (either the abbreviation, or the name) and transform the *other* into that standardized format.

Another example:  Suppose that two data sets contain weight measurements.  In the first, the units of measure are in pounds, and in the second, the units are in kilograms.  Again, a transformation is needed: Pick the standard unit you want to measure things in across your data sets, and transform the one(s) that aren't in conformity to that standard.

## Engineer Features
Akin to transformation, there are times when additional features (predictor variables, columns) can be created, or *engineered*.

At times, you'll use the existing features in the data set to make a calculation -- the calculation can be added to the overall data set as a new feature for the machine learning algorithm(s) you choose 

## Select Features

## Train Model

## Evaluate

## Deploy

## Verify

